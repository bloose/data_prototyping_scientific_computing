{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python parallel processing on Brown University cluster (Oscar).\n",
    "\n",
    "Use the documentation to supplement this guide https://docs.ccv.brown.edu/oscar/getting-started.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Learn how to request a parallel processor 'job'and set up a Python environment that will use that job.\n",
    "2. Write a short function to test and confirm that parallel processing is taking place.\n",
    "3. Use these concepts to modify landsatxplore.py from Week09 and implement the NDVI calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Log in to Oscar vis ssh. Request resources for your compute node.\n",
    "\n",
    "The [Oscar documentation](https://docs.ccv.brown.edu/oscar/submitting-jobs/slurm) describes how to request cluster computing resources or `jobs`, which are categorized into several distinct partitions. Oscar uses SLURM to manage and allocate resources, but we won't dedicate much time to understanding how SLURM works.  The Python library for distributed processing of array data (dask) will be the tool we focus on.\n",
    "\n",
    "The `interact` and `batch` commands allows us to request cluster jobs on Oscar.  Documentation for both is found here.  I think you can run all of your work using the `interact` command, as this will simlify the feedback and comprehension loop as you execute your code.  For interact, you need to request the number of nodes `-n`, the amount of time you want the job to last `-t`, and the RAM or memory `-m`.   \n",
    "\n",
    "The command below requests 5 processors and 150 GB of RAM for 60 minutes.  I find that the fewer processors you request, the faster your job is allocated to you.  **NOTE**: Please do not request more than 24 processors.\n",
    "\n",
    "~~~\n",
    "]$ interact -n 5 -t 00:60:00 -m 150g\n",
    "~~~\n",
    "\n",
    "~~~\n",
    "Cores:    5\n",
    "Walltime: 0:60:00\n",
    "Memory:   150g\n",
    "Queue:    batch\n",
    "salloc -J interact -N 1-1 -n 5 --time=0:60:00 --mem=150g -p batch srun --pty bash\n",
    "salloc: Pending job allocation 15699369\n",
    "salloc: job 15699369 queued and waiting for resources\n",
    "salloc: job 15699369 has been allocated resources\n",
    "salloc: Granted job allocation 15699369\n",
    "srun: Step created for job 15699369\n",
    "module: unloading 'java/8u111'\n",
    "module: loading 'java/8u111'\n",
    "module: unloading 'matlab/R2017b'\n",
    "module: loading 'matlab/R2017b'\n",
    "module: unloading 'python/3.7.4'\n",
    "module: loading 'python/2.7.12'\n",
    "module: unloading 'intel/2017.0'\n",
    "module: loading 'intel/2017.0'\n",
    "~~~\n",
    "\n",
    "**Note** that the modules we loaded (python 3.7 and anacond 3.5) get unloaded when the job is allocated. This is because, Oscar has assigned us a new compute `node` with the processing resources on the computer. Because we are occupying a different physical space in the cluster, our computing environment has been reset to the default.  Before we can do our work, we must reload those as below.\n",
    "\n",
    "~~~\n",
    "$ module load python/3.7.4\n",
    "$ module load anaconda/2020.02\n",
    "$ source /gpfs/runtime/opt/anaconda/2020.02/etc/profile.d/conda.sh\n",
    "$ source activate <your_env_here>\n",
    "~~~\n",
    "\n",
    "**Aside**.  You can include all of these commands into a text file called a shell script and just run the script to speed up the process.  The script must begin with the line `#!/usr/bin/bash`.  You can use nano to create this shell script.  The protocol is to give it the file extension `.sh`, ie `start.sh`.  After you have created the shell script you need to make it executable with `chmod` command.\n",
    "\n",
    "~~~\n",
    "$ chmod a+x start.sh\n",
    "~~~\n",
    "\n",
    "The script can be run at the command line using\n",
    "\n",
    "~~~\n",
    "$ source start.sh\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make a coreclock script to confirm parallel processing of computations.\n",
    "\n",
    "1. Download the script coreclock.py and examine the comments and contents.\n",
    "1. Add a module called coreclock() to the script, following the comments in the script.\n",
    "1. Upload the script to Oscar using sftp.\n",
    "1. Log in to oscar via ssh.\n",
    "1. Request compute resources for your job following step 2.\n",
    "1. Load modules, activate your conda environment.\n",
    "\n",
    "#### Notes about the code in coreclock.py\n",
    "\n",
    "~~~\n",
    "\t# Client() and LocalCluster() will be used to connect to the job resources that\n",
    "\t# were requested. \n",
    "\tfrom dask.distributed import Client, LocalCluster\n",
    "\t# Progress function reports the computation status to the screen\n",
    "\tfrom dask.distributed import progress\n",
    "\t# Use time library for sleep\n",
    "\timport time\n",
    "\t# Connect to resources.\n",
    "\tcluster = LocalCluster()\n",
    "\tjob = Client(cluster)\n",
    "\tprint(job)\n",
    "~~~\n",
    "\n",
    "The code block above creates a connection to the `interact` job.  The resources can be viewed with `print(job)` \n",
    "\n",
    "1. Use client.map() to execute coreclock() on 500 instances of x.\n",
    "1. Run coreclock.py at the command line:\n",
    "\n",
    "~~~\n",
    "$ python coreclock.py\n",
    "<Client: 'tcp://127.0.0.1:37750' processes=5 threads=5, memory=161.06 GB>\n",
    "[####                                    ] | 10% Completed | 11.6s\n",
    "~~~\n",
    "\n",
    "1. How long does it take for the code to complete execution?\n",
    "1. Based on 500 instances of coreclock() and the 1 second delay, how long would it take for a single processor to complete the same task?\n",
    "1. Does processing time scale proportionately with the number of cores? Consider rerunning coreclock with a different number of processors in your interact request, to see if the trend holds.\n",
    "1. Is parellel computation working as expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: What to turn in?\n",
    "\n",
    "* Answer the questions from Step 3 in this .ipynb.\n",
    "* Upload this .ipynb\n",
    "* Upload your modified version of coreclock.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
