{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python parallel processing on Brown University cluster (Oscar).\n",
    "\n",
    "Use the documentation to supplement this guide https://docs.ccv.brown.edu/oscar/getting-started.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Learn how to request a parallel processor 'job'and set up a Python environment that will use that job.\n",
    "2. Write a short function to test and confirm that parallel processing is taking place.\n",
    "3. Use these concepts to modify landsatxplore.py from Week09 and implement the NDVI calculation in the take home assignment.\n",
    "\n",
    "\n",
    "### Glossary of terms:\n",
    "\n",
    "1. Parallel processing:  Division of a calculation into pieces that can be computed at the same time or in parallel.\n",
    "1. Processor or worker: The CPU, GPU or TPU chip that performs the actual computations.\n",
    "1. Cluster:  A collection of processors or workers that are networked and orchestrated to carry out a parallel computation.\n",
    "1. Array: A 1, 2, 3, or N-dimensional data structure that can hold floats,or ints.\n",
    "1. Chunks:  The blocks or subsets of an array that Dask uses to break up large arrays for parallel processing.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dask for Parallel Processing.\n",
    "Dask is a flexible library for parallel computing in Python.\n",
    "\n",
    "We use Dask to **(1)** manage the cores in our cluster job, and **(2)** break up large arrays into chunks for parallel processing.\n",
    "\n",
    "1. To manage the processors or workers in the cluster. \n",
    "http://distributed.dask.org/en/stable/quickstart.html\n",
    "\n",
    "The Dask Distributed Library creates a client() instance that contains access to the existing cluster workers that have been allocated by the `interact` statement, listed below in Step 1.  Two functions .map() and .submit() tell Dask to parallelize the computation\n",
    "\n",
    "2. To divide up the LandSat array images into chunks that can be dealt to the parallel processor.\n",
    "\n",
    "<img src=\"https://bloose.github.io/data_prototyping_scientific_computing/images/dask-array-black-text.svg\" width=600> (c) Dask Core Developers <br><br>\n",
    "<img src=\"https://bloose.github.io/data_prototyping_scientific_computing/images/dask-dataframe.svg\" width=400> (c) Dask Core Developers\n",
    "\n",
    "\n",
    "### Arrays implement the NumPy API\n",
    "~~~python\n",
    "import dask.array as da\n",
    "x = da.random.random(size=(10000, 10000),\n",
    "                     chunks=(1000, 1000))\n",
    "x + x.T - x.mean(axis=0)\n",
    "~~~\n",
    "\n",
    "Watch this video for a brief introduction to Dask Array computations.\n",
    "[https://youtu.be/9h_61hXCDuI](https://youtu.be/9h_61hXCDuI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Log in to Oscar vis ssh. Request resources for your compute node.\n",
    "\n",
    "The [Oscar documentation](https://docs.ccv.brown.edu/oscar/submitting-jobs/slurm) describes how to request cluster computing resources or `jobs`, which are categorized into several distinct partitions. Oscar uses SLURM to manage and allocate resources, but we won't dedicate much time to understanding how SLURM works.  The Python library for distributed processing of array data (dask) will be the tool we focus on.\n",
    "\n",
    "The `interact` and `batch` commands allows us to request cluster jobs on Oscar.  Documentation for both is found here.  I think you can run all of your work using the `interact` command, as this will simlify the feedback and comprehension loop as you execute your code.  For interact, you need to request the number of nodes `-n`, the amount of time you want the job to last `-t`, and the RAM or memory `-m`.   \n",
    "\n",
    "The command below requests 5 processors and 150 GB of RAM for 60 minutes.  I find that the fewer processors you request, the faster your job is allocated to you.  **NOTE**: Please do not request more than 24 processors.\n",
    "\n",
    "~~~\n",
    "]$ interact -n 5 -t 00:60:00 -m 150g\n",
    "~~~\n",
    "\n",
    "~~~\n",
    "Cores:    5\n",
    "Walltime: 0:60:00\n",
    "Memory:   150g\n",
    "Queue:    batch\n",
    "salloc -J interact -N 1-1 -n 5 --time=0:60:00 --mem=150g -p batch srun --pty bash\n",
    "salloc: Pending job allocation 15699369\n",
    "salloc: job 15699369 queued and waiting for resources\n",
    "salloc: job 15699369 has been allocated resources\n",
    "salloc: Granted job allocation 15699369\n",
    "srun: Step created for job 15699369\n",
    "module: unloading 'java/8u111'\n",
    "module: loading 'java/8u111'\n",
    "module: unloading 'matlab/R2017b'\n",
    "module: loading 'matlab/R2017b'\n",
    "module: unloading 'python/3.7.4'\n",
    "module: loading 'python/2.7.12'\n",
    "module: unloading 'intel/2017.0'\n",
    "module: loading 'intel/2017.0'\n",
    "~~~\n",
    "\n",
    "**Note** that the modules we loaded (python 3.7 and anacond 3.5) get unloaded when the job is allocated. This is because, Oscar has assigned us a new compute `node` with the processing resources on the computer. Because we are occupying a different physical space in the cluster, our computing environment has been reset to the default.  Before we can do our work, we must reload those as below.\n",
    "\n",
    "~~~\n",
    "$ module load python/3.7.4\n",
    "$ module load anaconda/2020.02\n",
    "$ source /gpfs/runtime/opt/anaconda/2020.02/etc/profile.d/conda.sh\n",
    "$ source activate <your_env_here>\n",
    "~~~\n",
    "\n",
    "**Aside**.  You can include all of these commands into a text file called a shell script and just run the script to speed up the process.  The script must begin with the line `#!/usr/bin/bash`.  You can use nano to create this shell script.  The protocol is to give it the file extension `.sh`, ie `start.sh`.  After you have created the shell script you need to make it executable with `chmod` command.\n",
    "\n",
    "~~~\n",
    "$ chmod a+x start.sh\n",
    "~~~\n",
    "\n",
    "The script can be run at the command line using\n",
    "\n",
    "~~~\n",
    "$ source start.sh\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make a coreclock script to confirm parallel processing of computations.\n",
    "\n",
    "1. Download the script coreclock.py and examine the comments and contents.\n",
    "1. Add a module called coreclock() to the script, following the comments in the script.\n",
    "1. Upload the script to Oscar using sftp.\n",
    "1. Log in to oscar via ssh.\n",
    "1. Request compute resources for your job following step 2.\n",
    "1. Load modules, activate your conda environment.\n",
    "\n",
    "#### Notes about the code in coreclock.py\n",
    "\n",
    "~~~\n",
    "\t# Client() and LocalCluster() will be used to connect to the job resources that\n",
    "\t# were requested. \n",
    "\tfrom dask.distributed import Client, LocalCluster\n",
    "\t# Progress function reports the computation status to the screen\n",
    "\tfrom dask.distributed import progress\n",
    "\t# Use time library for sleep\n",
    "\timport time\n",
    "\t# Connect to resources.\n",
    "\tcluster = LocalCluster()\n",
    "\tjob = Client(cluster)\n",
    "\tprint(job)\n",
    "~~~\n",
    "\n",
    "The code block above creates a connection to the `interact` job.  The resources can be viewed with `print(job)` \n",
    "\n",
    "1. Use client.map() to execute coreclock() on 500 instances of x.\n",
    "1. Run coreclock.py at the command line:\n",
    "\n",
    "~~~\n",
    "$ python coreclock.py\n",
    "<Client: 'tcp://127.0.0.1:37750' processes=5 threads=5, memory=161.06 GB>\n",
    "[####                                    ] | 10% Completed | 11.6s\n",
    "~~~\n",
    "\n",
    "1. How long does it take for the code to complete execution?\n",
    "1. Based on 500 instances of coreclock() and the 1 second delay, how long would it take for a single processor to complete the same task?\n",
    "1. Does processing time scale proportionately with the number of cores? Consider rerunning coreclock with a different number of processors in your interact request, to see if the trend holds.\n",
    "1. Is parellel computation working as expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: What to turn in?\n",
    "\n",
    "* Answer the questions from Step 3 in this .ipynb.\n",
    "* Upload this .ipynb\n",
    "* Upload your modified version of coreclock.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
