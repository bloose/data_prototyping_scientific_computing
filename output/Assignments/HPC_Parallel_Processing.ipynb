{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python parallel processing on Unity HPC.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Learn how to request a parallel processor 'job'and set up a Python environment that will use that job.\n",
    "2. Write a short function to test and confirm that parallel processing is taking place (in-class).\n",
    "3. Use these concepts to modify landsatexplore.py from Week09 and implement the NDVI calculation (take-home).\n",
    "1. Use the python library Dask to extend Numpy array and Pandas dataframe calculations over multiple p\n",
    "\n",
    "Dask documentation explains how Dask works with parallel computing: https://tutorial.dask.org/02_array.html\n",
    "\n",
    "Use Unity documentation to understand more using HPC resources:https://docs.unity.uri.edu/documentation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Log in to Unity via OOD.\n",
    "\n",
    "Use your login info to connect to https://ood.unity.rc.umass.edu/ as before.\n",
    "\n",
    "\n",
    "**First**, load your conda environment, following the same steps as in Week09.\n",
    "\n",
    "~~~\n",
    "$ module load anaconda/2022.10\n",
    "$ source activate your_env_here\n",
    "~~~\n",
    "Remember to type  `$ source deactivate` to close your Anaconda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Request resources for your compute node.\n",
    "\n",
    "The [Unity documentation](https://docs.unity.uri.edu/documentation/jobs/salloc/) describes how to request cluster computing resources or `jobs`, which are categorized into several distinct partitions. Unity uses SLURM to manage and allocate resources, but we won't dedicate much time to understanding how SLURM works.  The Python library for distributed processing of array data (dask) will be the tool we focus on.\n",
    "\n",
    "The `salloc` command allows to request cluster jobs on Unity.  Documentation for both is linked above. For `salloc`, you can request the number of cpu nodes `-c`, the amount of time you want the job to last `--time`, and the RAM or memory `-mem`, and which partition to use `-p`.  A simple request looks like this:\n",
    "\n",
    "~~~\n",
    "]$ salloc -c 5     # Request 5 cpu nodes\n",
    "~~~\n",
    "\n",
    "The command below requests 12 cpus and 150 GB of RAM for 60 minutes on the partition called 'cpu'.  I find that the fewer processors you request, the faster your job is allocated to you.  **NOTE**: Please do not request more than 24 processors.\n",
    "\n",
    "~~~\n",
    "]$ salloc -J interact -c 12 --time=0:60:00 --mem=150G -p cpu  \n",
    "~~~\n",
    "\n",
    "**Note** that the module and environemnt we loaded (Anaconda) get unloaded when the job is allocated. This is because, Unity has assigned us a new compute `node` with the processing resources on the computer. Because we are occupying a different physical space in the cluster, our computing environment has been reset to the default.  Before we can do our work, we must reload those as below.\n",
    "\n",
    "\n",
    "**Aside**.  You can include all of these commands into a text file called a shell script and just run the script to speed up the process.  The script must begin with the line `#!/usr/bin/bash`.  You can use nano to create this shell script.  The protocol is to give it the file extension `.sh`, ie `start.sh`.  After you have created the shell script you need to make it executable with `chmod` command.\n",
    "\n",
    "~~~\n",
    "$ chmod a+x start.sh\n",
    "~~~\n",
    "\n",
    "The script can be run at the command line using\n",
    "\n",
    "~~~\n",
    "$ source start.sh\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Make a coreclock script to confirm parallel processing of computations.\n",
    "\n",
    "1. Download the script coreclock.py and examine the comments and contents.\n",
    "1. Add a module called coreclock() to the script, following the comments in the script.\n",
    "1. Upload the script to Unity.\n",
    "1. Open the Unity OOD Shell.\n",
    "1. Request compute resources for your job following Step 2.\n",
    "1. Load modules, activate your conda environment.\n",
    "\n",
    "#### Notes about the code in coreclock.py\n",
    "\n",
    "~~~\n",
    "\t# Client() and LocalCluster() will be used to connect to the job resources that\n",
    "\t# were requested. \n",
    "\tfrom dask.distributed import Client, LocalCluster\n",
    "\t# Progress function reports the computation status to the screen\n",
    "\tfrom dask.distributed import progress\n",
    "\t# Use time library for sleep\n",
    "\timport time\n",
    "\t# Connect to resources.\n",
    "\tcluster = LocalCluster()\n",
    "\tjob = Client(cluster)\n",
    "\tprint(job)\n",
    "~~~\n",
    "\n",
    "The code block above creates a connection to the `salloc` resources that were requested before starting python.  The resources can be viewed with `print(job)` \n",
    "\n",
    "1. Use client.map() to execute coreclock() 500 times and distribute them over the requested cpus.\n",
    "1. Run coreclock.py at the command line:\n",
    "\n",
    "~~~\n",
    "$ python coreclock.py\n",
    "<Client: 'tcp://127.0.0.1:37750' processes=5 threads=5, memory=161.06 GB>\n",
    "[####                                    ] | 10% Completed | 11.6s\n",
    "~~~\n",
    "\n",
    "1. How long does it take for the code to complete execution?\n",
    "1. Based on 500 instances of coreclock() and the 1 second delay, how long would it take for a single processor to complete the same task?\n",
    "1. Is parellel computation working as expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: What to turn in?\n",
    "\n",
    "* Answer the questions from Step 3 in this .ipynb.\n",
    "* Upload this .ipynb\n",
    "* Upload your modified version of coreclock.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
